{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Feature Alignment with Soft DTW Loss\n",
    "\n",
    "In this exercise we will use a library to get a distance function on time-series and use that as loss. You will get to know dynamic time warping which can be used to measure the distance between sequences. You will also use a certain network architecture called Siamese network which can be used supervised but also unsupervised to map positive and negative examples into a feature space and then minimize the distance of their feature representation. In this notebook you will use both together to train a network unsupervised to create a representation where similar activities are close to each other and finally use a kNN classifier to calculate the performance on a downstream classification task. \n",
    "\n",
    "### Exercise Overview\n",
    "\n",
    "In this exercise, you will:\n",
    "1. Use a library for **SOFT** Dynamic Time Warping (DTW) to compute time-series similarity. (Soft is very important as normal DTW is not differentiable). Please look into pysdtw (Available with pip or https://github.com/toinsson/pysdtw) which is a pytorch based Cuda-ready version.\n",
    "2. Implement a custom PyTorch Dataset for generating sequence pairs dynamically.\n",
    "3. Train a Siamese network using Soft DTW as the loss function.\n",
    "4. Evaluate the learned representations using a k-Nearest Neighbors (kNN) classifier.\n",
    "5. Finetune using a single linear layer\n",
    "6. Visualize and analyze the results.\n",
    "7. (Bonus) Show the strength between the correlation between the DTW-distance of the input and the feature representation.\n",
    "\n",
    "### Dataset\n",
    "We will use the UCI Human Activity Recognition (HAR) dataset, which contains time-series data from smartphone accelerometers and gyroscopes.\n",
    "\n",
    "**Important**: At the end you should write a report of adequate size, which will probably mean at least half a page. In the report you should describe how you approached the task. You should describe:\n",
    "- Encountered difficulties (due to the method, e.g. \"not enough training samples to converge\", not technical like \"I could not install a package over pip\")\n",
    "- Steps taken to alleviate difficulties\n",
    "- General description of what you did, explain how you understood the task and what you did to solve it in general language, no code.\n",
    "- Potential limitations of your approach, what could be issues, how could this be hard on different data or with slightly different conditions\n",
    "- If you have an idea how this could be extended in an interesting way, describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Time Warping\n",
    "Dynamic Time Warping (DTW) is an algorithm used to measure the similarity between two time series, even if they are out of sync in terms of speed or timing. Unlike traditional methods that align data point by point, DTW allows for non-linear alignment by \"warping\" the time axis. The idea is to find the optimal match between two sequences by stretching or compressing them along the time axis, minimizing the total distance between corresponding points. DTW does this by computing a cost matrix, where each entry represents the cost of aligning a point from one series with a point from the other. The path with the lowest cumulative cost is the optimal alignment.\n",
    "\n",
    "As a distance function, DTW is useful for comparing time series that might have different lengths or varying speeds. For example, DTW can be applied to applications such as speech recognition, where two spoken phrases might be of different lengths or spoken at different speeds, but still convey the same meaning. By calculating the DTW distance, we can measure how similar two time series are, regardless of time shifts or distortions. \n",
    "\n",
    "Excellent introduction: https://www.youtube.com/watch?v=ERKDHZyZDwA\n",
    "\n",
    "## Siamese Networks\n",
    "Siamese Networks are a type of neural network architecture designed for comparing two inputs and measuring their similarity. Instead of directly predicting a single label for an input, Siamese Networks take in two input data points, pass them through identical networks (hence \"siamese\"), and compare the outputs. The network can be trained by stating if the two inputs are equal or uneqal. If they map into a feature space instead of immediately into an output label space, the distance in the feature space can be used in the loss function which is the task here.\n",
    "\n",
    "In the context of this unsupervised representation learning, Siamese Networks can be used to learn meaningful features from unlabeled data. A popular technique related to this is *MoCo* (Momentum Contrast), which uses Siamese-like networks for contrastive learning. In MoCo, two different views (augmented versions) of the same data point are passed through two identical networks. One network is updated using the current model, while the other follows a momentum-based update rule. The networks are trained to bring the representations of similar views (positive pairs) closer together, while pushing the representations of dissimilar views (negative pairs) apart. This approach allows the model to learn useful representations for later fine-tuning on a classification task without needing explicit labels, relying instead on the assumption that augmented views of the same instance should be similar in the learned feature space.\n",
    "\n",
    "In this exercise the Siamese network should be used to learn to structure the feature space, guided by the similarity calculation of the DTW. After it should be evaluated how well that method performs for classification. The comment about MoCo is only for information about this close topic, it is not necessary to use it at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation\n",
    "\n",
    "Load the UCI HAR dataset and implement a custom `Dataset` class to generate pairs dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def unzip(filename, dest_path = None):\n",
    "    # unzips a zip file in the folder of the notebook to the notebook\n",
    "    with ZipFile(filename, 'r') as zObject: \n",
    "        # Extracting all the members of the zip  \n",
    "        # into a specific location. a\n",
    "        if dest_path is None:\n",
    "            zObject.extractall(path=os.getcwd())\n",
    "        else:\n",
    "            zObject.extractall(path=dest_path)\n",
    "\n",
    "import os\n",
    "def download(url, filename):\n",
    "    # download with check if file exists already\n",
    "    if os.path.isfile(filename):\n",
    "        return\n",
    "    urllib.request.urlretrieve(url,filename)\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Un-comment lines below only if executing on Google-COLAB\n",
    "# ![[ -f UCI_HAR.zip ]] || wget --no-check-certificate https://people.minesparis.psl.eu/fabien.moutarde/ES_MachineLearning/Practical_sequentialData/UCI_HAR.zip\n",
    "# ![[ -f \"UCI_HAR\" ]] || unzip UCI_HAR.zip\n",
    "\n",
    "download('https://people.minesparis.psl.eu/fabien.moutarde/ES_MachineLearning/Practical_sequentialData/UCI_HAR.zip','UCI_HAR.zip')\n",
    "\n",
    "unzip('UCI_HAR.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Load the UCI HAR dataset (adjust file paths as needed)\n",
    "X_train = pd.read_csv('./UCI_HAR/train/X_train.txt', delim_whitespace=True, header=None)\n",
    "y_train = pd.read_csv('./UCI_HAR/train/y_train.txt', header=None).values.ravel()\n",
    "X_test = pd.read_csv('./UCI_HAR/test/X_test.txt', delim_whitespace=True, header=None)\n",
    "y_test = pd.read_csv('./UCI_HAR/test/y_test.txt', header=None).values.ravel()\n",
    "\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# Define a custom Dataset for Siamese Network\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Define the logic for selecting pairs of sequences\n",
    "        pair_label = round(random.randint(0, 1))\n",
    "\n",
    "        if pair_label:\n",
    "            # Positive pair (same class)\n",
    "            idx2 = random.randint(0, len(self.data) - 1)\n",
    "            while self.labels[idx2] != self.labels[idx]:\n",
    "                idx2 = random.randint(0, len(self.data) - 1)\n",
    "\n",
    "        else:\n",
    "            # Negative pair (different class)\n",
    "            idx2 = random.randint(0, len(self.data) - 1)\n",
    "            while self.labels[idx2] == self.labels[idx]:\n",
    "                idx2 = random.randint(0, len(self.data) - 1)\n",
    "\n",
    "        seq1 = self.data.iloc[idx].values\n",
    "        seq2 = self.data.iloc[idx2].values\n",
    "        return torch.tensor(seq1, dtype=torch.float32), torch.tensor(seq2, dtype=torch.float32), pair_label\n",
    "\n",
    "# Create train and test datasets using the custom Dataset class\n",
    "train_dataset = SiameseDataset(X_train, y_train)\n",
    "test_dataset = SiameseDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Using a DTW Library\n",
    "\n",
    "Use a library to compute the Dynamic Time Warping (DTW) distance between sequences. Implement a differentiable Soft DTW function to calculate this distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and use a DTW library like pysdtw\n",
    "# Implement a function to calculate Soft DTW distance between sequences\n",
    "def dtw_distance(seq1, seq2):\n",
    "    # Add your implementation here\n",
    "    pass\n",
    "\n",
    "# Test the DTW function with example sequences\n",
    "seq1, seq2, label = train_dataset[0]\n",
    "distance = dtw_distance(seq1, seq2)\n",
    "print(f\"DTW distance: {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train a Siamese Network\n",
    "\n",
    "Define and train a Siamese network in PyTorch using the Soft DTW loss function. Implement the network structure and training logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Siamese Network class\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        # Initialize the network layers\n",
    "        pass\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        # Forward pass logic for one branch\n",
    "        pass\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # Logic for processing both branches\n",
    "        pass\n",
    "\n",
    "# Train the network with the defined loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluate Representations with kNN\n",
    "\n",
    "Use the trained network to extract embeddings and evaluate their quality using a kNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for train and test data\n",
    "# Use a kNN classifier to evaluate performance\n",
    "# Calculate and print accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Fine-Tuning with a Linear Layer\n",
    "\n",
    "Freeze the Siamese network and add a linear layer on top. Fine-tune the linear layer and re-evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the Siamese network\n",
    "# Add and train a linear layer for fine-tuning\n",
    "# Evaluate the fine-tuned model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
